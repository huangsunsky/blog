<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Solved Problems on cheon&#39;s blog</title>
    <link>https://www.cheon.site/blog/solved/</link>
    <description>Recent content in Solved Problems on cheon&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 04 May 2019 14:55:56 +0800</lastBuildDate>
    
	<atom:link href="https://www.cheon.site/blog/solved/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Proxy Settings</title>
      <link>https://www.cheon.site/blog/solved/proxy_settings/</link>
      <pubDate>Mon, 16 Dec 2019 16:40:16 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/proxy_settings/</guid>
      <description>Table of Contents  常见代理设置  git npm pip   常见代理设置 git git config --global http.proxy http://ip:port git config --global https.proxy http://ip:port 或者编辑 $HOME/.gitconfig:
[http] proxy = http://ip:port [https] proxy = http://ip:port 删除代理:
git config --global --unset http.proxy git config --global --unset https.proxy git 也支持 socks5 代理，把 http 替换为 socks5 即可。
npm 临时代理:
npm --registry=https://registry.npm.taobao.org install 全局配置:
npm config set registry https://registry.npm.taobao.org 或者编辑 $HOME/.npmrc:
registry=https://registry.npm.taobao.org 删除代理:
npm config delete registry pip 编辑 $HOME/.</description>
    </item>
    
    <item>
      <title>Ssh Config</title>
      <link>https://www.cheon.site/blog/solved/ssh_config/</link>
      <pubDate>Fri, 14 Jun 2019 17:22:12 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/ssh_config/</guid>
      <description>ssh 错误排查 新拿到的6台服务器，通过jumpserver用密钥登录到其中一台服务器，用 ssh 以账户密码的方式登录到其中另外一台服务器时遇见如下错误:
Permission denied (publickey,gssapi-keyex,gssapi-with-mic). 测试登录另外一台服务器，发现可以正常登录；测试登录自己，报相同的错误。可以判断应该是 ssh 服务端配置有问题。
经过排查，发现应该是服务端没有开启密码认证，修改/etc/ssh/sshd_config，修改为如下配置，将no替换为yes:
PasswordAuthentication yes 保存配置重新启动 sshd 服务:
systemctl restart sshd 再次测试发现已经可以登录了</description>
    </item>
    
    <item>
      <title>Connection Reset Error</title>
      <link>https://www.cheon.site/blog/solved/connection_reset_error/</link>
      <pubDate>Fri, 19 Oct 2018 18:03:25 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/connection_reset_error/</guid>
      <description>k8s 应用接口请求 connection reset 错误 测试人员在进行压力测试时发现应用A的接口出现了 connection reset 的错误，出错率大概为十分之一，猜想可能是代码问题或是网络问题或是连接数限制问题。
问题排查 应用A有三个环境，dev，uat和prod。其中dev和uat在同一集群中，因此网络环境相同，在代码上，三个环境都是一样的。
 测试prod环境的同一接口，并没有出现相同错误，说明代码没有问题。 如果是连接数限制问题，那么应该不止此次压测出现，而之前几次压测都没有出现改问题，所以暂时先排除。 验证网络问题，测试应用B的一个静态文件请求，也发现了相同的错误，现在基本可以确定是网络问题。  原因查找 要想排查网络问题，要先了解一下系统的网络架构。
首先最外面是一个A10负载均衡，80端口代理到k8s集群三个master节点的80端口，所有的域名都解析在这个A10上。
三台master节点的ip地址和80端口被用作集群中ingress controller的service的externalIP，域名通过ingress controller找到对应的应用。
网络问题需要一层一层排查，首先是应用本身。用循环来发送100次请求，查看应用日志：
for i in {1..100}; do curl http://www.test.com/api/example; echo $i;done 通过日志发现请求出现connection reset时，应用没有对应日志，说明请求没有到应用这里。
应用上一层是域名，域名是通过ingress来配置的，查看ingress controller的日志，发现也没有报错日志，所以请求也没有到这里。
接下来查看三台master节点的80端口，发现其中一台master1的80端口不通。看来问题应该就出现在这里了，我们可以手动来验证一下。
将要请求的域名映射为master2的ip地址，在/etc/hosts中加入www.test.com master2IP，进行测试，发现没有出现错误。改成master1的ip地址则发现无法连接。
问题解决 查看了k8s集群的网络组件，发现master1上的flannel一直处于container creating状态。将pod删除重启，再次进行测试，发现connection reset的错误已经没了。</description>
    </item>
    
    <item>
      <title>Redis Somaxconn</title>
      <link>https://www.cheon.site/blog/solved/redis_somaxconn/</link>
      <pubDate>Mon, 13 Aug 2018 11:07:55 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/redis_somaxconn/</guid>
      <description>kubernetes redis 最大连接数设置 应用在进行压力测试的时候发现请求多（5000并发）的时候会报出redis连接超时错误，查看了redis的配置，发现如下配置：
# TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 从这个配置可以看到，reids的最大连接数配置了511，所以应该将这个配置调高。注释里有提醒这个配置会受到linux内核配置的限制，查看/proc/sys/net/core/somaxconn，发现这个值只有128，所以redis配置tcp-backlog 511并没有生效，实际值只有128。
由于这里的redis是通过kubernetes部署的，所以需要同时修改宿主机和容器的内核参数。由于集群中有许多主机，所以我们通过为三台节点添加标签和污点来搭建redis等中间件应用并且防止其他应用部署到这些节点。通过查阅资料发现要修改k8s部署的容器的内核参数，需要开启kubelet的配置。所以具体操作分为四步：</description>
    </item>
    
    <item>
      <title>Firefox Open Markdown</title>
      <link>https://www.cheon.site/blog/solved/firefox_open_markdown/</link>
      <pubDate>Tue, 17 Apr 2018 01:17:33 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/firefox_open_markdown/</guid>
      <description>使用firefox打开并渲染markdown 如果用firefox打开markdown文件只有下载选项，需要更新mime的数据库:
在~/.local/share/mime/packages目录下创建text-markdown.xml文件，内容如下
&amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt; &amp;lt;mime-info xmlns=&amp;#39;http://www.freedesktop.org/standards/shared-mime-info&amp;#39;&amp;gt; &amp;lt;mime-type type=&amp;#34;text/plain&amp;#34;&amp;gt; &amp;lt;glob pattern=&amp;#34;*.md&amp;#34;/&amp;gt; &amp;lt;glob pattern=&amp;#34;*.mkd&amp;#34;/&amp;gt; &amp;lt;glob pattern=&amp;#34;*.markdown&amp;#34;/&amp;gt; &amp;lt;/mime-type&amp;gt; &amp;lt;/mime-info&amp;gt; 然后执行update-mime-database ~/.local/share/mime
完成后即可用firefox打开markdown文件。若想要查看markdown的渲染效果，可以安装markdown的插件，如markdow viewer webext等</description>
    </item>
    
    <item>
      <title>Openshift Restart Node</title>
      <link>https://www.cheon.site/blog/solved/openshift_restart_node/</link>
      <pubDate>Thu, 08 Mar 2018 18:09:41 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/openshift_restart_node/</guid>
      <description>openshift 节点无法连接集群 在openshift master 节点上执行oc get node:
$ oc get node NAME STATUS AGE node1 NotReady,SchedulingDisabled 301d master1 Ready 308d master2 Ready 308d master3 Ready 308d master4 Ready 308d master5 Ready 31d 其中的node1状态为NotReady,SchedulingDisabled，改节点没有准备好，并且是无法调度的，其中无法调度是手动设置的：
$ openshift admin manage-node node1 --schedulable=false 重启origin-node服务，让节点重新连接集群：
$ systemctl restart origin-node 重新查看节点状态：
$ oc get node NAME STATUS AGE node1 Ready,SchedulingDisabled 301d master1 Ready 308d master2 Ready 308d master3 Ready 308d master4 Ready 308d master5 Ready 31d 节点已经 ready，再将其设置为可调度的：</description>
    </item>
    
    <item>
      <title>Tomcat Deploy Error</title>
      <link>https://www.cheon.site/blog/solved/tomcat_deploy_error/</link>
      <pubDate>Tue, 06 Feb 2018 18:44:13 +0800</pubDate>
      
      <guid>https://www.cheon.site/blog/solved/tomcat_deploy_error/</guid>
      <description>tomcat部署war包出错 错误1 错误日志如下：
Caused by: java.lang.IllegalStateException: Unable to complete the scan for annotations for web application [/capitalplan] due to a StackOverflowError. Possible root causes include a too low setting for -Xss and illegal cyclic inheritance dependencies. The class hierarchy being processed was [org.bouncycastle.asn1.ASN1EncodableVector-&amp;gt;org.bouncycastle.asn1.DEREncodableVector-&amp;gt;org.bouncycastle.asn1.ASN1EncodableVector] at org.apache.catalina.startup.ContextConfig.checkHandlesTypes(ContextConfig.java:2099) at org.apache.catalina.startup.ContextConfig.processAnnotationsStream(ContextConfig.java:2043) at org.apache.catalina.startup.ContextConfig.processAnnotationsJar(ContextConfig.java:1989) at org.apache.catalina.startup.ContextConfig.processAnnotationsUrl(ContextConfig.java:1959) at org.apache.catalina.startup.ContextConfig.processAnnotations(ContextConfig.java:1912) at org.apache.catalina.startup.ContextConfig.webConfig(ContextConfig.java:1154) at org.apache.catalina.startup.ContextConfig.configureStart(ContextConfig.java:771) at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:298) at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:94) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5093) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:152) ... 10 more 解决方案：</description>
    </item>
    
  </channel>
</rss>